{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **ETL - Extract, Transform, Load Process**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import required libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell imports essential libraries for data manipulation (`pandas`, `numpy`), visualization (`matplotlib`, `seaborn`), and sets display options for better readability.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Optional settings for better display\n",
        "pd.set_option('display.max_columns', None)\n",
        "sns.set_theme(style='whitegrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Legend**  \n",
        "> `[AI-ASSIST: ChatGPT 06/08/2025]` – cell drafted/refactored with AI; author reviewed.  \n",
        "> `[AI-ASSIST: Copilot 06/08/2025]` – inline suggestion/bugfix; validated by author. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* This notebook performs the Extract, Transform, Load (ETL) process for the Diabetes dataset.\n",
        "We aim to load all raw CSV files, clean and transform the data where necessary, and prepare a combined dataset for analysis.\n",
        "\n",
        "- Extract multiple diabetes-related datasets from CSV files.\n",
        "- Merge these datasets into a single, unified DataFrame for streamlined analysis.\n",
        "- Clean and preprocess the combined data to prepare it for exploratory analysis and modeling.\n",
        "- Maintain reproducibility by properly managing working directories and file paths.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "This project uses three related CSV files containing health indicators and diabetes classification data from the 2015 Behavioral Risk Factor Surveillance System (BRFSS). Each file represents a slightly different version of the dataset for classification analysis.\n",
        "\n",
        "* `diabetes_012_health_indicators_BRFSS2015.csv`: Multiclass target diabetes data from the 2015 Behavioral Risk Factor Surveillance System (BRFSS).\n",
        "- `diabetes_binary_5050split_health_indicators_BRFSS2015.csv`: Balanced binary-class dataset, 50/50 split of diabetic and non-diabetic cases.\n",
        "- `diabetes_binary_health_indicators_BRFSS2015.csv`: Original binary classification dataset with an imbalanced class distribution.\n",
        "\n",
        "Location: All files are in the `data/` folder.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* A combined DataFrame merging all three datasets.\n",
        "- A cleaned CSV file (`combined_diabetes_data.csv`) saved for further use.\n",
        "- Prepared data ready for exploratory data analysis (EDA) and machine learning modeling.\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* All transformations will be documented and commented during each step. Column names and values will be standardised for consistency.\n",
        "- The three datasets vary slightly in class distribution and labeling, so adding a `source` column helps track their origin after merging.\n",
        "- Working directory adjustments ensure file paths remain consistent regardless of notebook location.\n",
        "- This notebook focuses on the ETL (Extract, Transform, Load) process; subsequent notebooks will handle EDA and modeling.\n",
        "- Dependencies such as pandas and os are used for data handling and file operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are ensuring the working directory is set to the project root so file paths (e.g. for reading data) work reliably.\n",
        "\n",
        "* We are assuming the notebooks will be stored in a subfolder, therefore when running the notebook in the editor, we will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working directory set to: /Users/nasraibrahim/Documents/vscode-projects\n"
          ]
        }
      ],
      "source": [
        "# This cell ensures the notebook is running from the project root so file paths work correctly. It changes the working directory to the parent folder of the notebook.\n",
        "import os\n",
        "\n",
        "# Change working directory to the project root (one level up from 'notebooks')\n",
        "os.chdir('..')\n",
        "print(\"Working directory set to:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/nasraibrahim/Documents/vscode-projects'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This cell prints the current working directory and lists files in the data folder to confirm setup.\n",
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/nasraibrahim/Documents'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /Users/nasraibrahim/Documents\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New working directory: /Users/nasraibrahim/Documents/vscode-projects/diabetes-data-analysis\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set this path to your actual project folder\n",
        "project_root = \"/Users/nasraibrahim/Documents/vscode-projects/diabetes-data-analysis\"\n",
        "\n",
        "os.chdir(project_root)\n",
        "print(\"New working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /Users/nasraibrahim/Documents/vscode-projects/diabetes-data-analysis\n",
            "Files in data folder: ['diabetes_012_health_indicators_BRFSS2015.csv', 'diabetes_binary_health_indicators_BRFSS2015.csv', 'diabetes_binary_5050split_health_indicators_BRFSS2015.csv']\n"
          ]
        }
      ],
      "source": [
        "# Confirm working directory and list files\n",
        "import os\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"Files in data folder:\", os.listdir('data'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1- Extract the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 1 content- \n",
        "\n",
        "In this section, we load the three diabetes-related CSV files stored in the `data` folder.  \n",
        "We first verify that all files exist, then read each into a Pandas DataFrame.  \n",
        "To keep track of the source of each record, a new column `source` is added to each DataFrame.  \n",
        "Finally, we combine all three datasets into a single DataFrame called `combined_df` for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /Users/nasraibrahim/Documents/vscode-projects/diabetes-data-analysis\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Diabetes_012</th>\n",
              "      <th>HighBP</th>\n",
              "      <th>HighChol</th>\n",
              "      <th>CholCheck</th>\n",
              "      <th>BMI</th>\n",
              "      <th>Smoker</th>\n",
              "      <th>Stroke</th>\n",
              "      <th>HeartDiseaseorAttack</th>\n",
              "      <th>PhysActivity</th>\n",
              "      <th>Fruits</th>\n",
              "      <th>...</th>\n",
              "      <th>GenHlth</th>\n",
              "      <th>MentHlth</th>\n",
              "      <th>PhysHlth</th>\n",
              "      <th>DiffWalk</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>Education</th>\n",
              "      <th>Income</th>\n",
              "      <th>source</th>\n",
              "      <th>Diabetes_binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>original</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>original</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>original</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>original</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>original</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
              "0           0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
              "1           0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
              "2           0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
              "3           0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
              "4           0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
              "\n",
              "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
              "0                   0.0           0.0     0.0  ...      5.0      18.0   \n",
              "1                   0.0           1.0     0.0  ...      3.0       0.0   \n",
              "2                   0.0           0.0     1.0  ...      5.0      30.0   \n",
              "3                   0.0           1.0     1.0  ...      2.0       0.0   \n",
              "4                   0.0           1.0     1.0  ...      2.0       3.0   \n",
              "\n",
              "   PhysHlth  DiffWalk  Sex   Age  Education  Income    source  Diabetes_binary  \n",
              "0      15.0       1.0  0.0   9.0        4.0     3.0  original              NaN  \n",
              "1       0.0       0.0  0.0   7.0        6.0     1.0  original              NaN  \n",
              "2      30.0       1.0  0.0   9.0        4.0     8.0  original              NaN  \n",
              "3       0.0       0.0  0.0  11.0        3.0     6.0  original              NaN  \n",
              "4       0.0       0.0  0.0  11.0        5.0     4.0  original              NaN  \n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This cell checks if each CSV file exists, loads them into DataFrames, adds a source column to track origin, and merges them into one DataFrame for analysis.\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "current_dir = os.getcwd()  # define current_dir first\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# File paths (use absolute paths based on current_dir)\n",
        "file_1 = os.path.join(current_dir, \"data/diabetes_012_health_indicators_BRFSS2015.csv\")\n",
        "file_2 = os.path.join(current_dir, \"data/diabetes_binary_5050split_health_indicators_BRFSS2015.csv\")\n",
        "file_3 = os.path.join(current_dir, \"data/diabetes_binary_health_indicators_BRFSS2015.csv\")\n",
        "\n",
        "# Check each file exists\n",
        "for f in [file_1, file_2, file_3]:\n",
        "    if not os.path.exists(f):\n",
        "        raise FileNotFoundError(f\"File not found: {f}\")\n",
        "\n",
        "# Load CSVs\n",
        "df1 = pd.read_csv(file_1)\n",
        "df2 = pd.read_csv(file_2)\n",
        "df3 = pd.read_csv(file_3)\n",
        "\n",
        "# Optional: Add a source column to keep track of origin\n",
        "df1['source'] = 'original'\n",
        "df2['source'] = 'balanced_5050'\n",
        "df3['source'] = 'binary'\n",
        "\n",
        "# Merge\n",
        "combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "\n",
        "# Preview\n",
        "combined_df.head()\n",
        "# AI-ASSIST: ChatGPT + Copilot (06/08/2025) — draft + refactor; author reviewed and tested.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2- Transform the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 2 content\n",
        "\n",
        "In this section, we perform necessary data cleaning and transformation steps on the combined dataset.  \n",
        "This includes handling missing values, converting data types if needed, and possibly creating new features.  \n",
        "The goal is to prepare a clean and consistent dataset for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values per column:\n",
            " Diabetes_012            324372\n",
            "HighBP                       0\n",
            "HighChol                     0\n",
            "CholCheck                    0\n",
            "BMI                          0\n",
            "Smoker                       0\n",
            "Stroke                       0\n",
            "HeartDiseaseorAttack         0\n",
            "PhysActivity                 0\n",
            "Fruits                       0\n",
            "Veggies                      0\n",
            "HvyAlcoholConsump            0\n",
            "AnyHealthcare                0\n",
            "NoDocbcCost                  0\n",
            "GenHlth                      0\n",
            "MentHlth                     0\n",
            "PhysHlth                     0\n",
            "DiffWalk                     0\n",
            "Sex                          0\n",
            "Age                          0\n",
            "Education                    0\n",
            "Income                       0\n",
            "source                       0\n",
            "Diabetes_binary         253680\n",
            "dtype: int64\n",
            "Data types:\n",
            " Diabetes_012            float64\n",
            "HighBP                  float64\n",
            "HighChol                float64\n",
            "CholCheck               float64\n",
            "BMI                     float64\n",
            "Smoker                  float64\n",
            "Stroke                  float64\n",
            "HeartDiseaseorAttack    float64\n",
            "PhysActivity            float64\n",
            "Fruits                  float64\n",
            "Veggies                 float64\n",
            "HvyAlcoholConsump       float64\n",
            "AnyHealthcare           float64\n",
            "NoDocbcCost             float64\n",
            "GenHlth                 float64\n",
            "MentHlth                float64\n",
            "PhysHlth                float64\n",
            "DiffWalk                float64\n",
            "Sex                     float64\n",
            "Age                     float64\n",
            "Education               float64\n",
            "Income                  float64\n",
            "source                   object\n",
            "Diabetes_binary         float64\n",
            "dtype: object\n",
            "Dropped 49740 duplicate rows.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Diabetes_012</th>\n",
              "      <th>HighBP</th>\n",
              "      <th>HighChol</th>\n",
              "      <th>CholCheck</th>\n",
              "      <th>BMI</th>\n",
              "      <th>Smoker</th>\n",
              "      <th>Stroke</th>\n",
              "      <th>HeartDiseaseorAttack</th>\n",
              "      <th>PhysActivity</th>\n",
              "      <th>Fruits</th>\n",
              "      <th>...</th>\n",
              "      <th>GenHlth</th>\n",
              "      <th>MentHlth</th>\n",
              "      <th>PhysHlth</th>\n",
              "      <th>DiffWalk</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>Education</th>\n",
              "      <th>Income</th>\n",
              "      <th>source</th>\n",
              "      <th>Diabetes_binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>original</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>original</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>original</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>original</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>original</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
              "0           0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
              "1           0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
              "2           0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
              "3           0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
              "4           0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
              "\n",
              "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  GenHlth  MentHlth  \\\n",
              "0                   0.0           0.0     0.0  ...      5.0      18.0   \n",
              "1                   0.0           1.0     0.0  ...      3.0       0.0   \n",
              "2                   0.0           0.0     1.0  ...      5.0      30.0   \n",
              "3                   0.0           1.0     1.0  ...      2.0       0.0   \n",
              "4                   0.0           1.0     1.0  ...      2.0       3.0   \n",
              "\n",
              "   PhysHlth  DiffWalk  Sex   Age  Education  Income    source  Diabetes_binary  \n",
              "0      15.0       1.0  0.0   9.0        4.0     3.0  original              0.0  \n",
              "1       0.0       0.0  0.0   7.0        6.0     1.0  original              0.0  \n",
              "2      30.0       1.0  0.0   9.0        4.0     8.0  original              0.0  \n",
              "3       0.0       0.0  0.0  11.0        3.0     6.0  original              0.0  \n",
              "4       0.0       0.0  0.0  11.0        5.0     4.0  original              0.0  \n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This cell checks for missing values, fills them (example for Diabetes_binary), prints data types, removes duplicate rows, and previews the cleaned data.\n",
        "# Check for missing values\n",
        "missing_counts = combined_df.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_counts)\n",
        "\n",
        "# Example: Fill missing values if appropriate (e.g., with mode or median)\n",
        "# Here we fill missing values in 'Diabetes_binary' with mode as example\n",
        "if combined_df['Diabetes_binary'].isnull().any():\n",
        "    mode_value = combined_df['Diabetes_binary'].mode()[0]\n",
        "    combined_df['Diabetes_binary'].fillna(mode_value, inplace=True)\n",
        "\n",
        "# Convert data types if needed (example)\n",
        "# combined_df['Age'] = combined_df['Age'].astype(int)\n",
        "\n",
        "# Check data types\n",
        "print(\"Data types:\\n\", combined_df.dtypes)\n",
        "\n",
        "# Optionally drop duplicate rows if any\n",
        "before_dedup = combined_df.shape[0]\n",
        "combined_df.drop_duplicates(inplace=True)\n",
        "after_dedup = combined_df.shape[0]\n",
        "print(f\"Dropped {before_dedup - after_dedup} duplicate rows.\")\n",
        "\n",
        "# Preview cleaned data\n",
        "combined_df.head()\n",
        "# AI-ASSIST: ChatGPT + Copilot (06/08/2025) — draft + refactor; author reviewed and tested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this stage of the ETL process, we performed essential data cleaning and transformation tasks to ensure the dataset is prepared for further analysis and modeling.\n",
        "\n",
        "✅ Actions Taken:\n",
        "Missing Value Analysis:\n",
        "\n",
        "We identified a significant number of missing values in two target-related columns:\n",
        "\n",
        "Diabetes_012: 324,372 missing values\n",
        "\n",
        "Diabetes_binary: 253,680 missing values\n",
        "\n",
        "No missing values were found in the remaining features.\n",
        "\n",
        "Duplicate Handling:\n",
        "\n",
        "A total of 49,740 duplicate rows were detected and removed from the dataset to ensure the integrity and uniqueness of the data.\n",
        "\n",
        "🔍 Observations:\n",
        "The missing values in Diabetes_012 and Diabetes_binary likely originate from how the datasets were merged — not all source files contained both target columns.\n",
        "\n",
        "All other health indicator features (such as HighBP, CholCheck, BMI, etc.) are fully populated and can be reliably used for further processing.\n",
        "\n",
        "The source column has been preserved to indicate the origin of each record, which may be useful for stratified analysis or understanding data distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 3 - Data Cleaning: Handling Missing Values and Duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we address common data quality issues:\n",
        "\n",
        "1. **Missing values** — These can lead to incorrect model training or analysis.\n",
        "    - We use different strategies to fill them:\n",
        "        - **Mean**: Useful for normally distributed numerical data.\n",
        "        - **Median**: Best for skewed numerical data (less sensitive to outliers).\n",
        "        - **Mode**: Best for categorical or ordinal features.\n",
        "2. **Duplicates** — Repeated rows can bias the results and should be removed.\n",
        "\n",
        "We'll apply these methods and save a fully cleaned version of our combined dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values per column BEFORE cleaning:\n",
            " Diabetes_012            298531\n",
            "HighBP                       0\n",
            "HighChol                     0\n",
            "CholCheck                    0\n",
            "BMI                          0\n",
            "Smoker                       0\n",
            "Stroke                       0\n",
            "HeartDiseaseorAttack         0\n",
            "PhysActivity                 0\n",
            "Fruits                       0\n",
            "Veggies                      0\n",
            "HvyAlcoholConsump            0\n",
            "AnyHealthcare                0\n",
            "NoDocbcCost                  0\n",
            "GenHlth                      0\n",
            "MentHlth                     0\n",
            "PhysHlth                     0\n",
            "DiffWalk                     0\n",
            "Sex                          0\n",
            "Age                          0\n",
            "Education                    0\n",
            "Income                       0\n",
            "source                       0\n",
            "Diabetes_binary              0\n",
            "dtype: int64\n",
            "\n",
            "✅ Dropped 0 duplicate rows.\n",
            "\n",
            "Missing values per column AFTER cleaning:\n",
            " Diabetes_012            0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "HeartDiseaseorAttack    0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             0\n",
            "GenHlth                 0\n",
            "MentHlth                0\n",
            "PhysHlth                0\n",
            "DiffWalk                0\n",
            "Sex                     0\n",
            "Age                     0\n",
            "Education               0\n",
            "Income                  0\n",
            "source                  0\n",
            "Diabetes_binary         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# This cell fills missing values in target columns using median/mode and removes duplicates, then prints the results to confirm the cleaning process.\n",
        "import pandas as pd\n",
        "\n",
        "# Display missing values before cleaning\n",
        "print(\"Missing values per column BEFORE cleaning:\\n\", combined_df.isnull().sum())\n",
        "\n",
        "# --- Handle missing values ---\n",
        "# Fill numeric column 'Diabetes_012' with median (skewed distribution)\n",
        "combined_df[\"Diabetes_012\"] = combined_df[\"Diabetes_012\"].fillna(combined_df[\"Diabetes_012\"].median())\n",
        "\n",
        "# Fill 'Diabetes_binary' with mode (binary classification target)\n",
        "combined_df[\"Diabetes_binary\"] = combined_df[\"Diabetes_binary\"].fillna(combined_df[\"Diabetes_binary\"].mode()[0])\n",
        "\n",
        "# Example: If you had a feature like 'Age' with missing values, you could use mean\n",
        "# combined_df[\"Age\"] = combined_df[\"Age\"].fillna(combined_df[\"Age\"].mean())\n",
        "\n",
        "# --- Remove duplicate rows ---\n",
        "before_dupes = combined_df.shape[0]\n",
        "combined_df = combined_df.drop_duplicates()\n",
        "after_dupes = combined_df.shape[0]\n",
        "print(f\"\\n✅ Dropped {before_dupes - after_dupes} duplicate rows.\")\n",
        "\n",
        "# Check missing values after cleaning\n",
        "print(\"\\nMissing values per column AFTER cleaning:\\n\", combined_df.isnull().sum())\n",
        "# AI-ASSIST: ChatGPT + Copilot (06/08/2025) — draft + refactor; author reviewed and tested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 4- Load the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we performed essential data cleaning tasks to prepare the dataset for analysis:\n",
        "\n",
        "Removed duplicate rows to ensure there are no redundant records that could bias the results.\n",
        "\n",
        "Handled missing values by imputing appropriate values depending on the data type:\n",
        "\n",
        "Used mean or median for numerical columns.\n",
        "\n",
        "Used mode for categorical columns.\n",
        "\n",
        "After cleaning, we validated the dataset by checking for any remaining missing values and duplicates to confirm that the data is now clean.\n",
        "\n",
        "Finally, the cleaned and validated dataset was saved as a new CSV file (combined_cleaned_final.csv) to preserve the cleaned state and avoid confusion with previous versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values after cleaning:\n",
            "Series([], dtype: int64)\n",
            "Number of duplicate rows remaining after dropping: 0\n"
          ]
        }
      ],
      "source": [
        "# This cell checks for any remaining missing values and duplicates, then saves the cleaned DataFrame to a new CSV file for downstream use.\n",
        "# Check for any remaining missing values per column\n",
        "missing_after = combined_df.isnull().sum()\n",
        "print(\"Missing values after cleaning:\")\n",
        "print(missing_after[missing_after > 0])  # Show only columns with missing values\n",
        "\n",
        "# Check for duplicates after dropping\n",
        "duplicates_after = combined_df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows remaining after dropping: {duplicates_after}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned and merged data saved to data/combined_cleaned_final.csv\n"
          ]
        }
      ],
      "source": [
        "# Define output file path for cleaned data\n",
        "output_file = \"data/combined_cleaned_final.csv\"\n",
        "\n",
        "# Ensure combined_df exists\n",
        "if 'combined_df' not in locals():\n",
        "    raise NameError(\"combined_df is not defined. Please run the cell where combined_df is created.\")\n",
        "\n",
        "# Save the cleaned DataFrame to CSV\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Cleaned and merged data saved to {output_file}\")\n",
        "# AI-ASSIST: ChatGPT + Copilot (06/08/2025) — draft + refactor; author reviewed and tested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusion and next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This ETL pipeline successfully extracted data from multiple sources, merged them into a unified dataset, and performed thorough cleaning to handle duplicates and missing values. The resulting dataset is now consistent, complete, and ready for further analysis.\n",
        "\n",
        "Next Steps:\n",
        "\n",
        "Feature Engineering: Enhance the dataset by creating new meaningful features that can improve model performance.\n",
        "\n",
        "Data Encoding: Apply techniques such as one-hot encoding to convert categorical variables into a machine-readable format.\n",
        "\n",
        "Scaling and Normalisation: Normalise numerical features to prepare for machine learning algorithms.\n",
        "\n",
        "Modeling and Evaluation: Use the cleaned and engineered data for predictive modeling and evaluate model effectiveness.\n",
        "\n",
        "Documentation: Continue documenting each step for clarity and reproducibility.\n",
        "\n",
        "This structured approach will ensure a smooth transition from data preparation to building robust machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attribution & AI Assistance\n",
        "\n",
        "Parts of this notebook (section layout, cleaning loops, join/merge patterns, export snippets) were assisted by **ChatGPT** and **GitHub Copilot**.  \n",
        "All code and outputs were reviewed, modified, and validated by the author. Any mistakes are my own.\n",
        "\n",
        "- Tools: ChatGPT (structure/markdown/code templates), Copilot (inline suggestions & refactors)  \n",
        "- Scope: NA handling, type casting, column standardisation, dataset exports  \n",
        "**Date:** 06 Aug 2025 • Author: Nassra Ibrahim"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
